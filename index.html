<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Beyond Scanpaths</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Beyond Scanpaths: Simulating Gaze with the Affinity Relation Transformer</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                Luke Palmer<sup>1*</sup>,</span>
                <span class="author-block">
                  Petar Palasek<sup>1*</sup>,</span>
                  <span class="author-block">
                    Hazem Abdelkawy<sup>2</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <sup>1</sup><a href="https://glimpse.ml" target="_blank">GlimpseML</a>,
                      <sup>2</sup><a href="https://www.toyota-europe.com/" target="_blank">Toyota Motor Europe</a>
                    </span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="#" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper (under review)</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->

                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->


                <span class="link-block">
                  <a href="" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon" style="vertical-align: middle; font-size: 20px;">&#129303;</span>
                      <span style="vertical-align: middle;">Dataset</span>
                  </a>
              </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="#" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>

                <!-- ArXiv abstract Link -->

                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/clipped_h264.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        On the left is ground-truth gaze data from six people, on the right are six simulations from our method. 
        We collected and make available a high quality multi-subject gaze dataset, the Focus100, and developed a spatio-temporal graph transformer approach to simulate continuous human-like gaze sequences.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Accurately modelling human attention is essential for numerous computer vision applications, particularly in the domain of automotive safety. Traditional methods simplify or aggregate gaze data into scanpath or saliency map representations, neglecting several aspects of natural gaze dynamics and potentially introducing artefacts into training data. Addressing these shortcomings, we introduce a dynamical systems approach that conceptualises human gaze as an active agent within the environment, allowing the modelling and generation of raw, continuous gaze data. We propose a gaze-centric spatiotemporal graph representation of driving scenes, processed by a novel heterogeneous graph transformer module (the Affinity Relation Transformer; ART), enabling dynamic modelling of interactions between driver gaze and surrounding traffic objects. Additionally, we introduce an Object-based mixture Density Network (ODN) for predicting next-step gaze distributions, accounting for the stochastic and object-centric nature of attentional shifts in complex environments. To support our approach, we also collected and make available the Focus100 dataset, featuring gaze from 30 engaged subjects viewing ego-centric driving footage. Our unified simulation-based approach demonstrates significant improvements in producing raw gaze sequences, spatial attention heatmaps, and naturalistic scanpath dynamics compared to state-of-the-art ego-centric gaze estimation approaches; offering valuable insights for the temporal modelling of attention and automotive safety.        </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Method -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          
          <h2 class="title is-3 has-text-centered">
            Affinity Relation Transformer + Object Density Network
          </h2>

          <img src="static/images/pipeline.png" 
          alt="Weight distance correlation" 
          class="blend-img-background center-image" 
          style="display: block; margin: 0 auto; max-width: 100%; height: auto;
                margin-bottom: 2rem;
                margin-top: 2rem;" />

          <div class="columns">
            <div class="column is-two-thirds">

              <p>We propose a novel framework for gaze prediction by modeling its spatiotemporal evolution as an active participant in the environment. Our approach leverages 
                graph-based simulation (GBS) to capture complex relationships by representing systems as graphs of objects/agents and their interactions, marking a first for 
                GBS in attention modeling. 
              </p>
              <p>
                A gaze-centric traffic graph (right), processed by our Affinity Relation graph Transformer (ART), captures dynamic interactions between 
                driver gaze and traffic objects, injecting relationship information into message passing. An Object-based Density Network (ODN) then predicts next-step 
                gaze distributions, accounting for the object-centric nature of attention shifts.
              </p>
            </div>
            <div class="column is-one-third">
              <img src="static/images/art_graph.png" alt="Weight distance correlation" class="blend-img-background center-image" style="max-width: 85%; height: auto;" />
              <em> Traffic scenes as heterogeneous graphs with nodes representing road structure, driving-relevant objects, and <strong>gaze</strong>. </em>
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<!-- Dataset -->
<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">

          <h2 class="title is-3 has-text-centered">
            Focus100 Dataset
          </h2>

          <figure>
            <img src="static/images/focus100.png" 
                 alt="Weight distance correlation" 
                 class="blend-img-background center-image" 
                 style="display: block; margin: 0 auto; max-width: 100%; height: auto;
                        margin-bottom: 1rem;
                        margin-top: 2rem;" />
            <figcaption class="has-text-centered">
              <em>The Focus100 dataset contains eye-tracking data from 30 subjects viewing 100 driving videos. The top row shows diversity in pedestrian traffic,
                 hazardousness, and road type. 
                 The bottom row shows the same video frame overlaid with the gaze samples of three separate subjects over the previous 2s; demonstrating 
                 the diversity of temporal gaze patterns for the same stimulus. </em>
            </figcaption>
          </figure>

              <p>
                Focus100 is a new dataset designed to facilitate research on dynamic human attention in 
                driving scenarios, particularly for the development and evaluation of gaze estimation models. 
                This dataset addresses critical limitations in existing driving gaze datasets, which often 
                lack raw temporal gaze data or sufficient scenario diversity.
              </p>
              <p>
                Focus100 features high-quality, multi-subject gaze data collected from 30 
                engaged subjects viewing 100 1-minute videos of ego-centric driving footage. 
                The dataset includes 15 hours of raw gaze sequences from at least 7 subjects per video, 
                providing valuable insights for temporal attention modeling and automotive safety.
              </p>

        </div>
      </div>
    </div>
  </div>
</section>

<!-- Results -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">

          <h2 class="title is-3 has-text-centered">
            Temporal Dynamics
          </h2>
          

            <img src="static/images/dynamics_example.png" 
                  alt="Weight distance correlation" 
                  class="blend-img-background center-image" 
                  style="display: block; margin: 0 auto; max-width: 100%; height: auto;
                        margin-bottom: 2rem;
                        margin-top: 2rem;" />

            <p>
                We compared our simulations to ground truth (human) and state-of-the-art ego-centric gaze estimation methods on the Focus100 dataset. 
                Above we see example gaze plots for each image dimension across a Focus100 video - each faint line is a subject/simulation and the fuller line is
                the mean sequence (AtTrans is deterministic and therefore only has one).
                Our method produces more human-like temporal gaze sequences than existing approaches; a result borne out across a range of 
                sequence and gaze similarity metrics (below).
            </p>


          <img src="static/images/dynamics_results.png" 
          alt="Weight distance correlation" 
          class="blend-img-background center-image" 
          style="display: block; margin: 0 auto; max-width: 100%; height: auto;
                margin-bottom: 2rem;
                margin-top: 2rem;" />
              
              
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<!-- Heatmap Results -->
<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">

          <h2 class="title is-3 has-text-centered">
            Heatmaps
          </h2>

          <video poster="" id="tree" autoplay controls muted loop height="100%"
                 style="display: block; margin: 0 auto; 
                        max-width: 100%; height: auto; 
                        margin-bottom: 2rem; margin-top: 2rem;">
            <!-- <source src="static/videos/1518011881342076_h264.mp4" -->
            <source src="static/videos/1518702770972457_h264.mp4"
            type="video/mp4">
          </video>

          <div class="columns">
            <div class="column is-two-thirds">
              <p>
                Using our generated simulation we are able to create other gaze representations, such as scanpaths and heatmaps.
                Above you can see heatmaps produced by ART simulations over a Focus100 test-set sequence (unseen during model training).
                The overlaid red circles are ground truth (human) gaze data.

                The results table on the right shows our method surpassing SOA appraoches - designed specifically for heatmap generation - on our dataset.
              </p>
            </div>
            <div class="column is-one-third">
              <img src="static/images/heatmap_results.png" alt="Weight distance correlation" class="blend-img-background center-image" style="max-width: 92%; height: auto;" />            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @misc{beyond-scanpaths,
          title={Beyond Scanpaths: Simulating Gaze with the Affinity Relation Transformer},
          author={Luke Palmer, Petar Palasek, Hazem Abdelkawy},
          year={2025}
        }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
